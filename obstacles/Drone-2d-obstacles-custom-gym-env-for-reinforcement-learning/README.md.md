# Drone 2D â€“ Obstacles (Gymnasium + SB3)

Ce dÃ©pÃ´t contient un environnement Gymnasium personnalisÃ© *drone-2d-custom-v0* et trois scripts principaux pour entraÃ®ner, Ã©valuer et comparer des agents PPO avec **Stable-Baselines3**.

> â„¹ï¸ Ã€ la racine, un dossier **`Eureka/`** contient Ã©galement la tentative rÃ©alisÃ©e avec le framework **Eureka** (expÃ©rimentations sÃ©parÃ©es).

## Table des matiÃ¨res
- [PrÃ©requis](#prÃ©requis)
- [Structure rapide](#structure-rapide)
- [Environnement & paramÃ¨tres clÃ©s](#environnement--paramÃ¨tres-clÃ©s)
- [Scripts principaux](#scripts-principaux)
  - [1) `examples/train.py`](#1-examplestrainpy)
  - [2) `examples/eval.py`](#2-exemplesevalpy)
  - [3) `examples/compare_agents.py`](#3-examplescompare_agentspy)
- [Conseils & bonnes pratiques](#conseils--bonnes-pratiques)

---

## PrÃ©requis
- Python 3.10+ recommandÃ©
- [Gymnasium](https://gymnasium.farama.org/) (API `reset() -> (obs, info)` et `step() -> (obs, reward, terminated, truncated, info)`)
- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/) (PPO)
- NumPy, TensorBoard

Installation indicativeÂ :
```bash
pip install -U gymnasium stable-baselines3 numpy tensorboard
# si l'environnement est packagÃ© localement, installez-le en editable
# depuis la racine du repo :
pip install -e ./drone_2d_custom_gym_env_package
```

> Les scripts ajoutent Ã©galement dynamiquement le chemin vers `drone_2d_custom_gym_env_package` via `sys.path.insert(...)`.

---

## Structure rapide
```
â”œâ”€ Obstacle_experiment/
â”‚  â”œâ”€ drone_2d_custom_gym_env_package/
â”‚  â”‚  â””â”€ (code de lâ€™environnement + enregistrement Gymnasium)
â”‚  â”œâ”€ examples/
â”‚  â”‚  â”œâ”€ train.py
â”‚  â”‚  â”œâ”€ eval.py
â”‚  â”‚  â””â”€ compare_agents.py
â”‚  â”œâ”€ agents/
â”‚  â”‚  â””â”€ (modÃ¨les appris .zip, checkpoints, etc.)
â”‚  â””â”€ (autres fichiers de configuration et logs)
â”‚
â””â”€ Eureka/
   â””â”€ (expÃ©rimentations avec le framework Eureka, reward functions gÃ©nÃ©rÃ©es, sessions dâ€™entraÃ®nement, logs, etc.)
```


---

## Environnement & paramÃ¨tres clÃ©s
Les trois scripts crÃ©ent lâ€™environnement ainsi (valeurs par dÃ©faut observÃ©es)Â :
```python
env = gym.make(
    "drone-2d-custom-v0",
    render_sim=False,        # ou True pour afficher la simulation
    render_path=False,       # trace du chemin
    render_shade=False,      # ombrage / zone de visibilitÃ©
    shade_distance=70,       # rayon de "shade"
    n_steps=500,             # horizon max par Ã©pisode
    n_fall_steps=10,         # pas aprÃ¨s lesquels on considÃ¨re une chute
    change_target=False,     # cible fixe ou changeante
    initial_throw=False,     # lancer initial
    use_obstacles=True,      # activer obstacles
    num_obstacles=3,         # nombre dâ€™obstacles
    fixed_map=True,          # carte fixe
    random_start=True        # position de dÃ©part alÃ©atoire
)
```
### Signification rapide
- **render_sim / render_path / render_shade**Â : contrÃ´le du rendu (coÃ»t CPU â†‘).
- **shade_distance**Â : portÃ©e de la zone dâ€™ombre/visibilitÃ©.
- **n_steps / n_fall_steps**Â : durÃ©e max Ã©pisode et dynamique de chute.
- **use_obstacles / num_obstacles**Â : obstacles activÃ©s et leur nombre.
- **fixed_map**Â : **doit rester Ã  `True`** â€” cette option garantit une carte figÃ©e.
  > âš ï¸ Mettre `fixed_map=False` gÃ©nÃ¨re des obstacles alÃ©atoires, mais cette option ne fonctionne pas correctement dans la version actuelle de lâ€™environnement.
- **random_start**Â : position de dÃ©part alÃ©atoire (meilleure robustesse).

> Les observations contiennent notamment des indicateurs de limites (ex. `obs[6]`, `obs[7]`), utilisÃ©s dans la comparaison pour dÃ©tecter les sorties de zone.

---

## Scripts principaux

### 1) `examples/train.py`
EntraÃ®ne un agent **PPO(MlpPolicy)** avec logs TensorBoard et gestion Ã©lÃ©gante de lâ€™arrÃªt manuel.

**Composants clÃ©sÂ :**
- **PPO("MlpPolicy", env, verbose=1, tensorboard_log="./tensorboard_logs/")**
- **Callback `KeyboardInterruptCallback`**
  - Sauvegarde un **checkpoint** toutes les **50â€¯000 Ã©tapes**Â : `checkpoint_agent_step_<n>`
  - En cas de `Ctrl+C`, sauve **immÃ©diatement** un modÃ¨leÂ : `checkpoint_agent_interrupted_<timestamp>`
  - Attributs importantsÂ :
    - `save_path` *(str)*Â : prÃ©fixe des fichiers de sauvegarde (`checkpoint_agent` par dÃ©faut).
- **Gestion de signal (`signal.SIGINT`)**Â : dÃ©clenche la sauvegarde finale via le callback.
- **DurÃ©e dâ€™entraÃ®nement**Â : `total_timesteps=3_000_000` (modifiable).
- **Sorties**Â : 
  - ModÃ¨le finalÂ : `final_agent`
  - Dossiers TensorBoardÂ : `./tensorboard_logs/`

**Lancer lâ€™entraÃ®nementÂ :**
```bash
python examples/train.py
# Visualiser ensuite :
tensorboard --logdir=./tensorboard_logs/
# Ouvrir http://localhost:6006
```

**Modifier les hyperparamÃ¨tres / lâ€™environnementÂ :**
- Ã‰ditez directement les arguments de `gym.make(...)` ou les paramÃ¨tres PPO.
- Changez `save_path`, `total_timesteps`, etc., dans le script.

---

### 2) `examples/eval.py`
Ã‰value un agent entraÃ®nÃ©, en mode continu ou Ã©pisode unique, avec rendu activable.

**Options en tÃªte de scriptÂ :**
```python
continuous_mode = True   # boucle sans fin dâ€™Ã©pisodes si True
random_action   = False  # pour jouer alÃ©atoirement (baseline)
use_fixed_map   = True   # rÃ©pÃ©ter sur mÃªme carte ou non
render_sim      = True   # afficher la simulation
```

**Chargement du modÃ¨leÂ :**
- Parcourt une **liste `model_paths`** et charge le **premier modÃ¨le existant** (ex. `../agents/final_agentRandomPosition.zip`, `final_agent.zip`, etc.).
- DÃ©finit une **graine alÃ©atoire** basÃ©e sur lâ€™horodatageÂ : `model.set_random_seed(...)`.

**Boucle dâ€™Ã©valuationÂ :**
- Action dÃ©terministeÂ : `model.predict(obs, deterministic=True)` (sauf si `random_action=True`).
- Comptabilise par Ã©pisodeÂ : **rÃ©compense totale** et **nombre dâ€™Ã©tapes**.
- En `continuous_mode=True`, lâ€™environnement est **rÃ©initialisÃ©** automatiquement Ã  chaque fin dâ€™Ã©pisode.

**ArrÃªt & statsÂ :**
- `Ctrl+C`Â : imprime le nombre dâ€™Ã©pisodes terminÃ©s + stats de lâ€™Ã©pisode en cours.
- Ferme toujours proprement lâ€™environnement (`env.close()`).

**Lancer lâ€™Ã©valuationÂ :**
```bash
python examples/eval.py
```

---

### 3) `examples/compare_agents.py`
Compare **deux agents PPO** sur un protocole identique, rÃ©pÃ©tÃ© `NUM_TRIALS` fois.

**ConfigurationÂ :**
```python
NUM_TRIALS = 30  # essais par agent
RENDER = False   # activer le rendu si besoin

agents = {
    "final_agentRandomPosition": "../agents/final_agentRandomPosition.zip",
    "final_agentWithoutObstacle": "../agents/final_agentWithoutObstacle.zip",
    # ajoutez dâ€™autres agents ici si besoin
}
```
> Si un chemin est manquant, le script le signale et continue.

**Protocole dâ€™Ã©valuationÂ :**
- Environnement **fixe** avec **obstacle central** et **positions de dÃ©part alÃ©atoires**.
- Par Ã©pisodeÂ :
  - action dÃ©terministe (`model.predict(..., deterministic=True)`)
  - **SuccÃ¨s** si la distance droneâ€“cible `< 30` (seuil).
  - Sinon, catÃ©gorisation de lâ€™Ã©checÂ :
    - **Collision obstacle** via `env.unwrapped.obstacle_manager.check_collision_with_drone(...)`
    - **Sortie de limites** via flags dâ€™observation (ex. `abs(obs[6]) == 1` ou `abs(obs[7]) == 1`)
    - **Timeout** si `episode_steps >= 500` et la distance reste â‰¥ 30.
- Statistiques par agentÂ :
  - `successes`, `collisions_obstacle`, `collisions_boundary`, `timeouts`
  - `avg_reward Â± std_reward`, `avg_steps Â± std_steps`
  - `success_rate = successes / NUM_TRIALS`

**Sortie & interprÃ©tationÂ :**
- Tableau console dÃ©taillÃ© par agent, puis **dÃ©termination du â€œgagnantâ€** via un score agrÃ©gÃ© (pondÃ©ration succÃ¨s/collisions/rapiditÃ©).  
- Imprime une **analyse**Â : qui rÃ©ussit plus souvent, Ã©vite mieux les obstacles, va plus vite, etc.

**Lancer la comparaisonÂ :**
```bash
python examples/compare_agents.py
```
---

## Agents

Il y a 3 agents entrainÃ©es dans le dossier ```agents/``` : 
- ```agent_finalWithoutObstacle```: l'agent entrainÃ© sans obstacles et des positions alÃ©atoires
- ```agent_finalWithObstacle```: l'agent entrainÃ© avec un obstacle et une position fixe
- ```agent_finalRandomPosition```: l'agent entrainÃ© avec un obstacle et des positions alÃ©atoires

Vous pouvez utilisez ceux ci dans les variables de eval.py ou compare_agents.py pour tester un petit les agents entraÃ®nÃ©s durant le projet.

---

## Dossier **Eureka/**

Un dossier **`Eureka/`** est prÃ©sent Ã  la racine du projet.  
Il contient notre tentative dâ€™utilisation du framework **Eureka**, dÃ©veloppÃ© par NVIDIA, pour gÃ©nÃ©rer automatiquement des fonctions de rÃ©compense grÃ¢ce Ã  un grand modÃ¨le de langage (LLM).

### RÃ©sultats obtenus
En pratique, **Eureka nâ€™a pas donnÃ© de bons rÃ©sultats** dans ce projet.  
La majoritÃ© des itÃ©rations ont obtenu **0â€¯% de success rate** lors de lâ€™Ã©valuation, malgrÃ© des **rÃ©compenses moyennes croissantes** pendant lâ€™entraÃ®nement.  
Une Ã©valuation a atteint **10â€¯%**, mais cela nâ€™est **pas statistiquement significatif**, car elle ne portait que sur **10 Ã©pisodes**.

> ğŸ† Meilleure fonction gÃ©nÃ©rÃ©e : `training_sessions/eureka_session_20251031_131836/reward_functions/reward_iter2.py`

### Pourquoi Eureka a Ã©chouÃ©
Lâ€™Ã©chec du modÃ¨le ne provient pas forcÃ©ment du framework lui-mÃªme, mais plutÃ´t dâ€™un **bug technique** survenu dans la pipeline dâ€™entraÃ®nement ou dâ€™Ã©valuation.  
Nous avons observÃ© plusieurs erreurs dans la gÃ©nÃ©ration automatique de code, notamment la **continuitÃ© du code aprÃ¨s un `return`** au lieu de rÃ©Ã©crire la fonction complÃ¨te.  
Il est Ã©galement possible quâ€™une **erreur de configuration** soit apparue lors de la fusion entre le projet Drone et le projet Eureka.  
Le **temps dâ€™entraÃ®nement trÃ¨s long** nâ€™a pas permis de corriger ces problÃ¨mes avant la date de rendu.

### Voies dâ€™amÃ©lioration
Pour amÃ©liorer les rÃ©sultats avec Eureka, il serait pertinent de :  
- Corriger la gÃ©nÃ©ration de code pour sâ€™assurer que chaque fonction de rÃ©compense soit rÃ©Ã©crite proprement.  
- Mettre en place une **vÃ©rification automatique** du code avant chaque entraÃ®nement.  
- Optimiser la **configuration et la durÃ©e des itÃ©rations** pour accÃ©lÃ©rer les cycles dâ€™apprentissage et fiabiliser les rÃ©sultats.

---
---

## Conseils & bonnes pratiques
- **Chemins de modÃ¨les**Â : stockez vos `.zip` dâ€™agents dans `agents/` et mettez Ã  jour `model_paths` / `agents`.
- **`fixed_map` vs `random_start`**Â : utilisez `fixed_map=True` pour des comparaisons cohÃ©rentes, mais gardez `random_start=True` pour tester la robustesse.
- **TensorBoard**Â : surveillez les mÃ©triques dâ€™apprentissage (`tensorboard_logs/`).

---

